from "stdlib/io.jou" import printf, putchar, FILE, fgetc, ferror, fopen
from "stdlib/str.jou" import sprintf, strlen, strchr, strcmp, starts_with
from "stdlib/mem.jou" import malloc, realloc, free, memset, memmove
from "./errors_and_warnings.jou" import Location, fail, assert
from "./token.jou" import Token, TokenKind

# TODO: move these to stdlib
declare atoi(s: byte*) -> int
declare isprint(b: int) -> int

def is_identifier_or_number_byte(b: byte) -> bool:
    return (
        ('A' <= b and b <= 'Z')
        or ('a' <= b and b <= 'z')
        or ('0' <= b and b <= '9')
        or b == '_'
    )

def is_operator_byte(c: byte) -> bool:
    return c != '\0' and strchr("=<>!.,()[]{};:+-*/&%", c) != NULL

def is_keyword(word: byte*) -> bool:
    # This keyword list is in 3 places. Please keep them in sync:
    #   - the Jou compiler written in C
    #   - self-hosted compiler
    #   - syntax documentation
    keywords = [
        "from", "import",
        "def", "declare", "class", "enum", "global",
        "return", "if", "elif", "else", "while", "for", "break", "continue",
        "True", "False", "NULL",
        "and", "or", "not", "as", "sizeof",
        "void", "bool", "byte", "int", "long", "float", "double",
    ]

    for i = 0; i < sizeof keywords / sizeof keywords[0]; i++:
        if strcmp(keywords[i], word) == 0:
            return True
    return False


class Tokenizer:
    f: FILE*
    location: Location
    pushback: byte*
    pushback_len: int  # TODO: dynamic array
    # Parens array isn't dynamic, so that you can't segfault
    # the compiler by feeding it lots of nested parentheses,
    # which would make it recurse too deep.
    parens: Token[50]
    parens_len: int

    def read_byte(self: Tokenizer*) -> byte:
        EOF = -1  # FIXME

        c: byte
        if self->pushback_len > 0:
            c = self->pushback[--self->pushback_len]
        else:
            temp = fgetc(self->f)
            if temp == '\r':
                # On Windows, \r just before \n is ignored.
                temp = fgetc(self->f)
                if temp != EOF and temp != '\n':
                    # TODO: test this, if possible?
                    fail(self->location, "source file contains a CR byte ('\\r') that isn't a part of a CRLF line ending")

            if temp == EOF:
                if ferror(self->f) != 0:
                    # TODO: include errno in the error message
                    fail(self->location, "cannot read file")
                # Use the zero byte to denote end of file.
                c = '\0'
            elif temp == '\0':
                # TODO: test this
                fail(self->location, "source file contains a zero byte")
                c = 'x'  # TODO: silences compiler warning, but never runs
            else:
                c = temp as byte

        if c == '\n':
            self->location.lineno++
        return c

    def unread_byte(self: Tokenizer*, b: byte) -> void:
        if b == '\0':
            return

        assert(b != '\r')
        self->pushback = realloc(self->pushback, self->pushback_len + 1)
        self->pushback[self->pushback_len++] = b
        if b == '\n':
            self->location.lineno--

    def read_identifier_or_number(self: Tokenizer*, first_byte: byte) -> byte[100]:
        dest: byte[100]
        memset(&dest, 0, sizeof dest)
        destlen = 0

        assert(is_identifier_or_number_byte(first_byte))
        dest[destlen++] = first_byte

        while True:
            b = self.read_byte()
            if is_identifier_or_number_byte(b):
                if destlen == sizeof dest - 1:
                    fail(self->location, "name or number is too long")
                dest[destlen++] = b
            else:
                self.unread_byte(b)
                return dest

    def consume_rest_of_line(self: Tokenizer*) -> void:
        while True:
            c = self.read_byte()
            if c == '\0' or c == '\n':
                self.unread_byte(c)
                break

    # Returns the indentation level for the next line
    def read_newline_token(self: Tokenizer*) -> int:
        level = 0
        while True:
            c = self.read_byte()
            if c == '\0':
                # End of file. Do not validate that indentation is a
                # multiple of 4 spaces. Add a trailing newline implicitly
                # if needed.
                #
                # TODO: test this
                return 0
            elif c == '\n':
                level = 0
            elif c == '#':
                self.consume_rest_of_line()
            elif c == ' ':
                level++
            else:
                self.unread_byte(c)
                return level

    # Assumes the initial ' has been read.
    def read_byte_literal(self: Tokenizer*) -> byte:
        c = self.read_byte()
        if c == '\'':
            fail(self->location, "a byte literal cannot be empty, maybe use double quotes to instead make a string?")
        if c == '\0' or c == '\n':
            if c == '\n':
                self->location.lineno--
            fail(self->location, "missing ' to end the byte literal")

        if c == '\\':
            after_backslash = self.read_byte()
            if after_backslash == '\0' or after_backslash == '\n':
                fail(self->location, "missing ' to end the byte literal")
            elif after_backslash == 'n':
                c = '\n'
            elif after_backslash == 'r':
                c = '\r'
            elif after_backslash == 't':
                c = '\t'
            elif after_backslash == '\\':
                c = '\\'
            elif after_backslash == '\'':
                c = '\''
            elif after_backslash == '"':
                fail(self->location, "double quotes shouldn't be escaped in byte literals")
            elif '0' <= after_backslash and after_backslash <= '9':
                c = after_backslash - '0'
            elif after_backslash < 0x80 and isprint(after_backslash) != 0:
                message: byte* = malloc(100)
                sprintf(message, "unknown escape: '\\%c'", after_backslash)
                fail(self->location, message)
            else:
                fail(self->location, "unknown '\\' escape")

        end = self.read_byte()
        if end != '\'':
            # If there's another single quote later on the same line, suggest using double quotes.
            location = self->location
            while True:
                c = self.read_byte()
                if c == '\0' or c == '\n':
                    break
                if c == '\'':
                    fail(location, "single quotes are for a single character, maybe use double quotes to instead make a string?")
            fail(location, "missing ' to end the byte literal")

        return c

    # Assumes the initial " has been read.
    def read_string(self: Tokenizer*) -> byte*:
        result: byte* = NULL
        len = 0

        while True:
            c = self.read_byte()
            if c == '"':
                break
            elif c == '\n' or c == '\0':
                if c == '\n':
                    self->location.lineno--
                fail(self->location, "missing \" to end the string")
            elif c == '\\':
                # \n means newline, for example
                after_backslash = self.read_byte()
                if after_backslash == '\0':
                    fail(self->location, "missing \" to end the string")
                elif after_backslash == 'n':
                    result = realloc(result, len+1)
                    result[len++] = '\n'
                elif after_backslash == 'r':
                    result = realloc(result, len+1)
                    result[len++] = '\r'
                elif after_backslash == 't':
                    result = realloc(result, len+1)
                    result[len++] = '\t'
                elif after_backslash == '\\' or after_backslash == '"':
                    result = realloc(result, len+1)
                    result[len++] = after_backslash
                elif after_backslash == '\'':
                    fail(self->location, "single quotes shouldn't be escaped in strings")
                elif after_backslash == '0':
                    fail(self->location, "strings cannot contain zero bytes (\\0), because that is the special end marker byte")
                elif '1' <= after_backslash and after_backslash <= '9':
                    result = realloc(result, len+1)
                    result[len++] = after_backslash - '0'
                elif after_backslash == '\n':
                    # \ at end of line, string continues on next line
                    len = len  # TODO: pass statement
                else:
                    if after_backslash < 0x80 and isprint(after_backslash) != 0:
                        message: byte* = malloc(100)
                        sprintf(message, "unknown escape: '\\%c'", after_backslash)
                        fail(self->location, message)
                    else:
                        fail(self->location, "unknown '\\' escape")
            else:
                result = realloc(result, len+1)
                result[len++] = c

        result = realloc(result, len+1)
        result[len] = '\0'
        return result

    def read_operator(self: Tokenizer*) -> byte[100]:
        operators = [
            # This list of operators is in 3 places. Please keep them in sync:
            #   - the Jou compiler written in C
            #   - self-hosted compiler
            #   - syntax documentation
            #
            # Longer operators are first, so that '==' does not tokenize as '=' '='
            "...", "===", "!==",
            "==", "!=", "->", "<=", ">=", "++", "--", "+=", "-=", "*=", "/=", "%=", "::",
            ".", ",", ":", ";", "=", "(", ")", "{", "}", "[", "]", "&", "%", "*", "/", "+", "-", "<", ">",
        ]

        operator: byte[100]
        memset(&operator, 0, sizeof operator)

        # Read as many operator characters as we may need.
        while strlen(&operator[0]) < 3:
            c = self.read_byte()
            if not is_operator_byte(c):
                self.unread_byte(c)
                break
            operator[strlen(&operator[0])] = c

        for i = 0; i < sizeof operators / sizeof operators[0]; i++:
            if starts_with(&operator[0], operators[i]):
                # Unread the bytes we didn't use.
                while strlen(&operator[0]) > strlen(operators[i]):
                    last = &operator[strlen(&operator[0]) - 1]
                    self.unread_byte(*last)
                    *last = '\0'

                # "===" and "!==" are here only to give a better error message to javascript people.
                if strcmp(&operator[0], "===") != 0 and strcmp(&operator[0], "!==") != 0:
                    return operator

        message: byte[100]
        sprintf(&message[0], "there is no '%s' operator", &operator[0])
        fail(self->location, &message[0])
        return operator  # TODO: never actually runs, but causes a compiler warning

    def read_token(self: Tokenizer*) -> Token:
        while True:
            token = Token{location = self->location}
            b = self.read_byte()

            if b == ' ':
                continue
            if b == '#':
                self.consume_rest_of_line()
                continue

            if b == '\n':
                if self->parens_len > 0:
                    continue
                token.kind = TokenKind::Newline
                token.indentation_level = self.read_newline_token()
            elif b == '"':
                token.kind = TokenKind::String
                token.long_string = self.read_string()
            elif b == '\'':
                token.kind = TokenKind::Byte
                token.byte_value = self.read_byte_literal()
            elif is_identifier_or_number_byte(b):
                token.short_string = self.read_identifier_or_number(b)
                if is_keyword(&token.short_string[0]):
                    token.kind = TokenKind::Keyword
                elif '0' <= token.short_string[0] and token.short_string[0] <= '9':
                    # TODO: support various other things
                    token.kind = TokenKind::Int
                    token.int_value = atoi(&token.short_string[0])
                else:
                    token.kind = TokenKind::Name
            elif is_operator_byte(b):
                self.unread_byte(b)
                token.kind = TokenKind::Operator
                token.short_string = self.read_operator()
            elif b == '\0':
                token.kind = TokenKind::EndOfFile
            else:
                message: byte[100]
                sprintf(&message[0], "unexpected byte %#02x", b)
                fail(self->location, &message[0])
            return token


def tokenize_without_indent_dedent_tokens(file: FILE*, path: byte*) -> Token*:
    tokenizer = Tokenizer{
        location = Location{path = path},
        f = file,
    }

    # Add a fake newline to the beginning. It does a few things:
    #  * Less special-casing: blank lines in the beginning of the file can
    #    cause there to be a newline token anyway.
    #  * It is easier to detect an unexpected indentation in the beginning
    #    of the file, as it becomes just like any other indentation.
    #  * Line numbers start at 1.
    tokenizer.pushback = malloc(1)
    tokenizer.pushback[0] = '\n'
    tokenizer.pushback_len = 1

    tokens: Token* = NULL
    len = 0
    while len == 0 or tokens[len-1].kind != TokenKind::EndOfFile:
        tokens = realloc(tokens, sizeof(tokens[0]) * (len+1))
        tokens[len++] = (&tokenizer).read_token()

    free(tokenizer.pushback)
    return tokens


# Creates a new array of tokens with indent/dedent tokens added after
# newline tokens that change the indentation level.
def handle_indentations(raw_tokens: Token*) -> Token*:
    tokens: Token* = NULL
    ntokens = 0
    level = 0

    for t = raw_tokens; True; t++:
        if t->kind == TokenKind::EndOfFile:
            # Add an extra newline token at end of file and the dedents after it.
            # This makes it similar to how other newline and dedent tokens work:
            # the dedents always come after a newline token.
            tokens = realloc(tokens, sizeof tokens[0] * (ntokens + level/4 + 1))
            while level != 0:
                tokens[ntokens++] = Token{location = t->location, kind = TokenKind::Dedent}
                level -= 4
            tokens[ntokens++] = *t
            break

        tokens = realloc(tokens, sizeof tokens[0] * (ntokens+1))
        tokens[ntokens++] = *t

        if t->kind == TokenKind::Newline:
            after_newline = t->location
            after_newline.lineno++

            if t->indentation_level % 4 != 0:
                fail(after_newline, "indentation must be a multiple of 4 spaces")

            while level < t->indentation_level:
                tokens = realloc(tokens, sizeof tokens[0] * (ntokens+1))
                tokens[ntokens++] = Token{location = after_newline, kind = TokenKind::Indent}
                level += 4

            while level > t->indentation_level:
                tokens = realloc(tokens, sizeof tokens[0] * (ntokens+1))
                tokens[ntokens++] = Token{location = after_newline, kind = TokenKind::Dedent}
                level -= 4

    # Delete the newline token in the beginning.
    #
    # If the file has indentations after it, they are now represented by separate
    # indent tokens and parsing will fail. If the file doesn't have any blank/comment
    # lines in the beginning, it has a newline token anyway to avoid special casing.
    assert(tokens[0].kind == TokenKind::Newline)
    memmove(&tokens[0], &tokens[1], sizeof tokens[0] * (ntokens - 1))

    return tokens


def tokenize(path: byte*) -> Token*:
    file = fopen(path, "rb")
    if file == NULL:
        # TODO: test this
        # TODO: include errno in the message
        fail(Location{path=path}, "cannot open file")

    raw_tokens = tokenize_without_indent_dedent_tokens(file, path)
    better_tokens = handle_indentations(raw_tokens)
    free(raw_tokens)
    return better_tokens

def print_tokens(tokens: Token*) -> void:
    printf("===== Tokens for file \"%s\" =====\n", tokens->location.path)
    t = tokens
    current_lineno = -1

    while True:
        if t->location.lineno != current_lineno:
            current_lineno = t->location.lineno
            printf("\nLine %d:\n", current_lineno)

        printf("  ")
        t.print()

        if t->kind == TokenKind::EndOfFile:
            break
        t++

    printf("\n")

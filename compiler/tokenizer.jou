# NOTE: This file is performance critical, especially for `jou --check`. If you
#       modify this file more than a little bit, please measure the performance
#       of `jou --check compiler/main.jou` with and without your changes.

import "stdlib/assert.jou"
import "stdlib/io.jou"
import "stdlib/list.jou"
import "stdlib/str.jou"
import "stdlib/utf8.jou"
import "stdlib/mem.jou"
import "stdlib/errno.jou"
import "stdlib/ascii.jou"
import "./errors_and_warnings.jou"
import "./token.jou"

def is_identifier_or_number_byte(b: byte) -> bool:
    # Allows non-ASCII variable names (bytes 128-255)
    return b >= 128 or is_ascii_letter(b) or is_ascii_digit(b) or b == '_'

def is_operator_byte(c: byte) -> bool:
    return c != '\0' and strchr("=<>!.,()[]{};:+-*/&%|^~", c) != NULL

def is_keyword(word: byte*) -> bool:
    # See also: tests/should_succeed/keywords.jou
    # Let's avoid unnecessary strcmp() calls because they are slow.
    match word[0]:
        case 'F':
            return strcmp(word, "False") == 0
        case 'N':
            return strcmp(word, "NULL") == 0 or strcmp(word, "None") == 0
        case 'T':
            return strcmp(word, "True") == 0
        case 'a':
            return (
                strcmp(word, "and") == 0
                or strcmp(word, "as") == 0
                or strcmp(word, "assert") == 0
                or strcmp(word, "array_count") == 0
            )
        case 'b':
            return (
                strcmp(word, "break") == 0
                or strcmp(word, "bool") == 0
                or strcmp(word, "byte") == 0
            )
        case 'c':
            return (
                strcmp(word, "class") == 0
                or strcmp(word, "const") == 0
                or strcmp(word, "continue") == 0
                or strcmp(word, "case") == 0
            )
        case 'd':
            return (
                strcmp(word, "def") == 0
                or strcmp(word, "declare") == 0
                or strcmp(word, "double") == 0
            )
        case 'e':
            return (
                strcmp(word, "elif") == 0
                or strcmp(word, "else") == 0
                or strcmp(word, "enum") == 0
                or strcmp(word, "enum_count") == 0
            )
        case 'f':
            return (
                strcmp(word, "for") == 0
                or strcmp(word, "float") == 0
                or strcmp(word, "funcptr") == 0
            )
        case 'g':
            return strcmp(word, "global") == 0
        case 'i':
            return (
                strcmp(word, "if") == 0
                or strcmp(word, "import") == 0
                or strcmp(word, "int") == 0
                or strcmp(word, "int8") == 0
                or strcmp(word, "int16") == 0
                or strcmp(word, "int32") == 0
                or strcmp(word, "int64") == 0
            )
        case 'l':
            return strcmp(word, "link") == 0
        case 'm':
            return strcmp(word, "match") == 0
        case 'n':
            return strcmp(word, "not") == 0 or strcmp(word, "noreturn") == 0
        case 'o':
            return strcmp(word, "or") == 0
        case 'p':
            return strcmp(word, "pass") == 0
        case 'r':
            return strcmp(word, "return") == 0
        case 's':
            return strcmp(word, "self") == 0 or strcmp(word, "sizeof") == 0
        case 't':
            return strcmp(word, "typedef") == 0
        case 'u':
            return (
                strcmp(word, "union") == 0
                or strcmp(word, "uint8") == 0
                or strcmp(word, "uint16") == 0
                or strcmp(word, "uint32") == 0
                or strcmp(word, "uint64") == 0
            )
        case 'v':
            return strcmp(word, "void") == 0
        case 'w':
            return strcmp(word, "while") == 0 or strcmp(word, "with") == 0
        case _:
            return False

def hexdigit_value(c: byte) -> int:
    if 'A' <= c and c <= 'F':
        return 10 + (c - 'A')
    if 'a' <= c and c <= 'f':
        return 10 + (c - 'a')
    if '0' <= c and c <= '9':
        return c - '0'
    return -1

def parse_integer(string: byte*, location: Location) -> uint64:
    if starts_with(string, "0b"):
        base = 2
        digits = &string[2]
        valid_digits = "01_"
    elif starts_with(string, "0o"):
        base = 8
        digits = &string[2]
        valid_digits = "01234567_"
    elif starts_with(string, "0x"):
        base = 16
        digits = &string[2]
        valid_digits = "0123456789ABCDEFabcdef_"
    else:
        # default decimal number
        base = 10
        digits = string
        valid_digits = "0123456789_"

    if (
        strlen(digits) == 0
        or digits[0] == '_'
        or digits[strlen(digits) - 1] == '_'
        or strspn(digits, valid_digits) != strlen(digits)
    ):
        message: byte*
        asprintf(&message, "invalid number or variable name \"%s\"", string)
        fail(location, message)

    if base == 10 and starts_with(string, "0") and strlen(string) >= 2:
        # wrong syntax like 0777
        fail(location, "unnecessary zero at start of number")

    # TODO: use strtoull() or similar? This isn't too bad written by hand though.
    result = 0 as uint64
    overflow = False

    for i = 0; i < strlen(digits); i++:
        if digits[i] == '_':
            continue
        digit_value = hexdigit_value(digits[i]) as uint64

        # Overflow isn't UB in Jou
        if (result * (base as uint64)) / (base as uint64) != result:
            overflow = True
            break
        result *= base as uint64

        if result + digit_value < result:
            overflow = True
            break
        result += digit_value

    if overflow:
        fail(location, "value does not fit into any supported integer type")

    return result


def is_valid_double(str: byte*) -> bool:
    # See doc/syntax-spec.md
    e = strstr(str, "e")
    if e == NULL:
        # One or more digits, then '.', then one or more digits
        n1 = strspn(str, "0123456789")
        if str[n1] != '.':
            return False
        n2 = strspn(&str[n1 + 1], "0123456789")
        return n1 > 0 and n2 > 0 and n1 + 1 + n2 == strlen(str)
    else:
        # First anything accepted above or just one or more digits
        n1 = strspn(str, "0123456789")
        if &str[n1] != e:
            if str[n1] != '.':
                return False
            n2 = strspn(&str[n1 + 1], "0123456789")
            if n1 == 0 or n2 == 0 or &str[n1 + 1 + n2] != e:
                return False
        # Then e, then possibly minus, then one or more digits
        p = e
        p++
        if *p == '-':
            p++
        return *p != '\0' and strspn(p, "0123456789") == strlen(p)

def is_valid_float(str: byte[100]) -> bool:
    n = strlen(str)
    if n == 0 or (str[n-1] != 'F' and str[n-1] != 'f'):
        return False
    str[n-1] = '\0'
    return is_valid_double(str) or strspn(str, "0123456789") == strlen(str)


def flip_paren(c: byte) -> byte:
    match c:
        case '(':
            return ')'
        case ')':
            return '('
        case '[':
            return ']'
        case ']':
            return '['
        case '{':
            return '}'
        case '}':
            return '{'
        case _:
            assert False


class Tokenizer:
    file_content_ptr: byte*
    file_content_start: byte*
    location: Location
    # Parens array isn't dynamic, so that you can't segfault
    # the compiler by feeding it lots of nested parentheses,
    # which would make it recurse too deep.
    open_parens: Token[50]
    open_parens_len: int

    # TODO: Get rid of read_byte() / unread_byte()?
    def read_byte(self) -> byte:
        match *self.file_content_ptr:
            case '\0':
                return '\0'
            case '\n':
                self.location.lineno++
                return *self.file_content_ptr++
            case _:
                return *self.file_content_ptr++

    def unread_byte(self, b: byte) -> None:
        if b == '\0':
            return

        assert self.file_content_ptr > self.file_content_start
        assert self.file_content_ptr[-1] == b
        self.file_content_ptr--
        if b == '\n':
            self.location.lineno--

    def read_identifier_or_number(self, first_byte: byte) -> byte[100]:
        dest: byte[100]
        memset(&dest, 0, sizeof dest)
        destlen = 0

        assert is_identifier_or_number_byte(first_byte) or first_byte == '@'
        dest[destlen++] = first_byte
        is_number = '0' <= first_byte and first_byte <= '9'

        while True:
            b = self.read_byte()
            if (
                is_identifier_or_number_byte(b)
                or (is_number and (b == '.' or (b == '-' and dest[destlen-1] == 'e')))
            ):
                if destlen == sizeof dest - 1:
                    if is_number:
                        template = "number is too long: %.20s..."
                    else:
                        template = "name is too long: %.20s..."
                    message: byte[100]
                    sprintf(message, template, dest)
                    fail(self.location, message)
                dest[destlen++] = b
            else:
                self.unread_byte(b)
                return dest

    def consume_rest_of_line(self) -> None:
        while True:
            c = self.read_byte()
            if c == '\0' or c == '\n':
                self.unread_byte(c)
                break

    # Returns the indentation level for the next line
    def read_newline_token(self) -> int:
        level = 0
        while True:
            c = self.read_byte()
            match c:
                case '\0':
                    # End of file. Do not validate that indentation is a
                    # multiple of 4 spaces. Add a trailing newline implicitly
                    # if needed.
                    return 0
                case '\n':
                    level = 0
                case '#':
                    self.consume_rest_of_line()
                case ' ':
                    level++
                case _:
                    self.unread_byte(c)
                    return level

    def read_hex_escape_byte(self) -> byte:
        n1 = hexdigit_value(self.read_byte())
        n2 = hexdigit_value(self.read_byte())
        if n1 == -1 or n2 == -1:
            fail(self.location, "\\x must be followed by two hexadecimal digits (0-9, A-F) to specify a byte")
        return (n1*16 + n2) as byte

    # Assumes the initial ' has been read.
    def read_byte_literal(self) -> byte:
        c = self.read_byte()
        if c == '\'':
            fail(self.location, "a byte literal cannot be empty, maybe use double quotes to instead make a string?")
        if c == '\0' or c == '\n':
            if c == '\n':
                self.location.lineno--
            fail(self.location, "missing ' to end the byte literal")

        if c == '\\':
            after_backslash = self.read_byte()
            match after_backslash:
                case '\0':
                    fail(self.location, "missing ' to end the byte literal")
                case '\n':
                    self.location.lineno--
                    fail(self.location, "missing ' to end the byte literal")
                case 'n':
                    c = '\n'
                case 'r':
                    c = '\r'
                case 't':
                    c = '\t'
                case '\\':
                    c = '\\'
                case '\'':
                    c = '\''
                case '"':
                    fail(self.location, "double quotes shouldn't be escaped in byte literals")
                case '0':
                    c = '\0'
                case 'x':
                    c = self.read_hex_escape_byte()
                case _:
                    if is_ascii_printable(after_backslash):
                        message: byte* = malloc(100)
                        sprintf(message, "unknown escape: '\\%c'", after_backslash)
                        fail(self.location, message)
                    else:
                        fail(self.location, "unknown '\\' escape")

        end = self.read_byte()
        if end != '\'':
            # If there's another single quote later on the same line, suggest using double quotes.
            location = self.location
            while True:
                match self.read_byte():
                    case '\0' | '\n':
                        fail(location, "missing ' to end the byte literal")
                    case '\'':
                        fail(location, "single quotes are for specifying a byte, maybe use double quotes to instead make a string?")

        return c

    # Assumes the initial " has been read.
    def read_string(self) -> byte*:
        result = List[byte]{}
        msg: byte[100]

        while True:
            c = self.read_byte()
            match c:
                case '"':
                    break
                case '\n':
                    self.location.lineno--
                    fail(self.location, "missing \" to end the string")
                case '\0':
                    fail(self.location, "missing \" to end the string")
                case '\\':
                    # \n means newline, for example
                    after_backslash = self.read_byte()
                    match after_backslash:
                        case '\0':
                            fail(self.location, "missing \" to end the string")
                        case 'n':
                            result.append('\n')
                        case 'r':
                            result.append('\r')
                        case 't':
                            result.append('\t')
                        case '\\':
                            result.append('\\')
                        case '"':
                            result.append('"')
                        case '\'':
                            fail(self.location, "single quotes shouldn't be escaped in strings")
                        case '0':
                            fail(self.location, "strings cannot contain zero bytes (\\0), because that is the special end marker byte")
                        case 'x':
                            b = self.read_hex_escape_byte()
                            if b == '\0':
                                fail(self.location, "strings cannot contain zero bytes (\\x00), because that is the special end marker byte")
                            result.append(b)
                        case '\n':
                            # \ at end of line, string continues on next line
                            pass
                        case _:
                            if is_ascii_printable(after_backslash):
                                snprintf(msg, sizeof(msg), "unknown escape: '\\%c'", after_backslash)
                            else:
                                msg = "unknown '\\' escape"
                            fail(self.location, msg)
                case _:
                    result.append(c)

        result.append('\0')
        return result.ptr

    def read_operator(self) -> byte[100]:
        assert is_operator_byte(*self.file_content_ptr)

        if starts_with(self.file_content_ptr, "==="):
            fail(self.location, "use '==' instead of '==='")
        if starts_with(self.file_content_ptr, "!=="):
            fail(self.location, "use '!=' instead of '!=='")
        if starts_with(self.file_content_ptr, "&&"):
            fail(self.location, "use 'and' instead of '&&'")
        if starts_with(self.file_content_ptr, "||"):
            fail(self.location, "use 'or' instead of '||'")
        if self.file_content_ptr[0] == '!' and self.file_content_ptr[1] != '=':
            fail(self.location, "use 'not' instead of '!'")

        result: byte[100] = ""

        # Please update the syntax documentation if you add new operators.
        # Longer operators are first, so that '==' does not tokenize as '=' '='
        if (
            starts_with(self.file_content_ptr, "<<=")
            or starts_with(self.file_content_ptr, ">>=")
            or starts_with(self.file_content_ptr, "...")
        ):
            len = 3
        elif (
            starts_with(self.file_content_ptr, "==")
            or starts_with(self.file_content_ptr, "!=")
            or starts_with(self.file_content_ptr, "->")
            or starts_with(self.file_content_ptr, "<=")
            or starts_with(self.file_content_ptr, ">=")
            or starts_with(self.file_content_ptr, "++")
            or starts_with(self.file_content_ptr, "--")
            or starts_with(self.file_content_ptr, "+=")
            or starts_with(self.file_content_ptr, "-=")
            or starts_with(self.file_content_ptr, "*=")
            or starts_with(self.file_content_ptr, "/=")
            or starts_with(self.file_content_ptr, "%=")
            or starts_with(self.file_content_ptr, "&=")
            or starts_with(self.file_content_ptr, "|=")
            or starts_with(self.file_content_ptr, "^=")
            or starts_with(self.file_content_ptr, "<<")
            or starts_with(self.file_content_ptr, ">>")
        ):
            len = 2
        else:
            # This assumes that any valid operator character is valid as a
            # single-character operator. Currently that is true for everything
            # except '!' which was already handled above.
            len = 1

        dest = &result[0]
        while len --> 0:
            *dest++ = *self.file_content_ptr++
        *dest = '\0'
        return result

    def handle_parentheses(self, token: Token*) -> None:
        msg: byte[100]

        if token.kind == TokenKind.EndOfFile and self.open_parens_len > 0:
            open_token = self.open_parens[0]
            actual_open = open_token.short_string[0]
            expected_close = flip_paren(actual_open)
            snprintf(msg, sizeof(msg), "'%c' without a matching '%c'", actual_open, expected_close)
            fail(open_token.location, msg)

        if token.is_open_paren():
            if self.open_parens_len == sizeof self.open_parens / sizeof self.open_parens[0]:
                fail(token.location, "too many nested parentheses")
            self.open_parens[self.open_parens_len++] = *token

        if token.is_close_paren():
            actual_close = token.short_string[0]
            expected_open = flip_paren(actual_close)
            if self.open_parens_len == 0 or self.open_parens[--self.open_parens_len].short_string[0] != expected_open:
                snprintf(msg, sizeof(msg), "'%c' without a matching '%c'", actual_close, expected_open)
                fail(token.location, msg)

    def read_token(self) -> Token:
        while True:
            token = Token{location = self.location, file_content_ptr = self.file_content_ptr}
            b = self.read_byte()
            match b:
                case ' ':
                    continue
                case '#':
                    self.consume_rest_of_line()
                    continue
                case '\n':
                    if self.open_parens_len > 0:
                        continue
                    token.kind = TokenKind.Newline
                    token.indentation_level = self.read_newline_token()
                case '"':
                    token.kind = TokenKind.String
                    token.long_string = self.read_string()
                case '\'':
                    token.kind = TokenKind.Byte
                    token.integer_value = self.read_byte_literal()
                case '@':
                    token.kind = TokenKind.Decorator
                    token.short_string = self.read_identifier_or_number(b)
                case '\t':
                    fail(self.location, "Jou files cannot contain tab characters (use 4 spaces for indentation)")
                case '\0':
                    token.kind = TokenKind.EndOfFile
                case _:
                    if is_identifier_or_number_byte(b):
                        token.short_string = self.read_identifier_or_number(b)
                        if is_keyword(token.short_string):
                            token.kind = TokenKind.Keyword
                        elif '0' <= token.short_string[0] and token.short_string[0] <= '9':
                            if is_valid_double(token.short_string):
                                token.kind = TokenKind.Double
                            elif is_valid_float(token.short_string):
                                token.short_string[strlen(token.short_string) - 1] = '\0'  # delete trailing 'f' or 'F'
                                token.kind = TokenKind.Float
                            else:
                                token.kind = TokenKind.Integer
                                token.integer_value = parse_integer(token.short_string, token.location)
                        else:
                            token.kind = TokenKind.Name
                    elif is_operator_byte(b):
                        self.unread_byte(b)
                        token.kind = TokenKind.Operator
                        token.short_string = self.read_operator()
                    else:
                        message: byte[100]
                        if is_ascii_printable(b):
                            sprintf(message, "unexpected byte '%c' (%#02x)", b, b)
                        else:
                            sprintf(message, "unexpected byte %#02x", b)
                        fail(self.location, message)

            self.handle_parentheses(&token)
            return token


def tokenize_without_indent_dedent_tokens(path: byte*, file_content: byte*) -> Token*:
    tokenizer = Tokenizer{
        location = Location{path = path},
        file_content_ptr = file_content,
        file_content_start = file_content,
    }

    tokens = List[Token]{}
    while tokens.len == 0 or tokens.end()[-1].kind != TokenKind.EndOfFile:
        tokens.append(tokenizer.read_token())
    return tokens.ptr


# Creates a new array of tokens with indent/dedent tokens added after
# newline tokens that change the indentation level.
def handle_indentations(raw_tokens: Token*) -> Token*:
    tokens = List[Token]{}
    level = 0

    for t = raw_tokens; True; t++:
        if t.kind == TokenKind.EndOfFile:
            # Add an extra newline token at end of file and the dedents after it.
            # This makes it similar to how other newline and dedent tokens work:
            # the dedents always come after a newline token.
            if t > raw_tokens and t[-1].kind != TokenKind.Newline:
                tokens.append(Token{location = t.location, file_content_ptr = t.file_content_ptr, kind = TokenKind.Newline})
            while level != 0:
                tokens.append(Token{location = t.location, file_content_ptr = t.file_content_ptr, kind = TokenKind.Dedent})
                level -= 4
            tokens.append(*t)
            break

        tokens.append(*t)

        if t.kind == TokenKind.Newline:
            after_newline = t.location
            after_newline.lineno++

            if t.indentation_level % 4 != 0:
                fail(after_newline, "indentation must be a multiple of 4 spaces")

            while level < t.indentation_level:
                tokens.append(Token{location = after_newline, file_content_ptr = &t.file_content_ptr[1], kind = TokenKind.Indent})
                level += 4

            while level > t.indentation_level:
                tokens.append(Token{location = after_newline, file_content_ptr = &t.file_content_ptr[1], kind = TokenKind.Dedent})
                level -= 4

    # Delete the newline token in the beginning.
    #
    # If the file has indentations after it, they are now represented by separate
    # indent tokens and parsing will fail. If the file doesn't have any blank/comment
    # lines in the beginning, it has a newline token anyway to avoid special casing.
    assert tokens.ptr[0].kind == TokenKind.Newline
    memmove(&tokens.ptr[0], &tokens.ptr[1], sizeof tokens.ptr[0] * --tokens.len)

    return tokens.ptr


def read_file_to_string(path: byte*, import_location: Location*) -> byte*:
    file = fopen(path, "rb")
    if file == NULL:
        message: byte[200]
        if import_location == NULL:
            # File is not imported
            snprintf(message, sizeof message, "cannot open file: %s", strerror(get_errno()))
            fail(Location{path=path}, message)
        else:
            snprintf(message, sizeof message, "cannot import from \"%s\": %s", path, strerror(get_errno()))
            fail(*import_location, message)

    bytes_list = List[byte]{}

    # Add a fake newline to the beginning. It does a few things:
    #  * Less special-casing: blank lines in the beginning of the file can
    #    cause there to be a newline token anyway.
    #  * It is easier to detect an unexpected indentation in the beginning
    #    of the file, as it becomes just like any other indentation.
    #  * Line numbers start at 1.
    bytes_list.append('\n')

    buf: byte[4096]
    while True:
        num_read = fread(buf, 1, sizeof buf, file)
        if num_read == 0:
            if ferror(file) != 0:
                # TODO: include errno in the error message?
                # TODO: test this?
                fail(Location{path = path}, "cannot read file")
            break
        bytes_list.extend_from_ptr(buf, num_read)
    fclose(file)

    # Make sure we have room to add a zero byte later.
    bytes_list.grow(bytes_list.len + 1)

    location = Location{path = path}
    dest = bytes_list.ptr
    for src = bytes_list.ptr; src < bytes_list.end(); src++:
        match *src:
            case '\0':
                fail(location, "source file contains a zero byte")
            case '\r':
                # Handle \r\n line endings
                if &src[1] < bytes_list.end() and src[1] == '\n':
                    continue  # Delete \r from string
                # TODO: test this, if possible?
                fail(location, "source file contains a CR byte ('\\r') that isn't a part of a CRLF line ending")
            case '\n':
                location.lineno++
        *dest++ = *src

    *dest = '\0'
    return bytes_list.ptr


class BadChar:
    codepoint: uint32
    name: byte*


def detect_bad_chars(path: byte*, file_content: byte*) -> None:
    # Ban all UTF-8 whitespace characters (longest one is 3 bytes)
    # Python code used to generate this:
    #
    #    import unicodedata
    #    for x in range(128, 0x110000):  # 128 to skip ascii, chr(0x110000) errors
    #        if chr(x).isspace():
    #            try:
    #                name = unicodedata.name(chr(x))
    #            except ValueError:
    #                name = "???"
    #            print('BadChar{codepoint = %#08x, name = "%s"},' % (x, name))
    bad_chars = [
        BadChar{codepoint = 0x00000085, name = "NEXT LINE (NEL)"},   # name added manually
        BadChar{codepoint = 0x000000a0, name = "NO-BREAK SPACE"},
        BadChar{codepoint = 0x00001680, name = "OGHAM SPACE MARK"},
        BadChar{codepoint = 0x00002000, name = "EN QUAD"},
        BadChar{codepoint = 0x00002001, name = "EM QUAD"},
        BadChar{codepoint = 0x00002002, name = "EN SPACE"},
        BadChar{codepoint = 0x00002003, name = "EM SPACE"},
        BadChar{codepoint = 0x00002004, name = "THREE-PER-EM SPACE"},
        BadChar{codepoint = 0x00002005, name = "FOUR-PER-EM SPACE"},
        BadChar{codepoint = 0x00002006, name = "SIX-PER-EM SPACE"},
        BadChar{codepoint = 0x00002007, name = "FIGURE SPACE"},
        BadChar{codepoint = 0x00002008, name = "PUNCTUATION SPACE"},
        BadChar{codepoint = 0x00002009, name = "THIN SPACE"},
        BadChar{codepoint = 0x0000200a, name = "HAIR SPACE"},
        BadChar{codepoint = 0x00002028, name = "LINE SEPARATOR"},
        BadChar{codepoint = 0x00002029, name = "PARAGRAPH SEPARATOR"},
        BadChar{codepoint = 0x0000202f, name = "NARROW NO-BREAK SPACE"},
        BadChar{codepoint = 0x0000205f, name = "MEDIUM MATHEMATICAL SPACE"},
        BadChar{codepoint = 0x00003000, name = "IDEOGRAPHIC SPACE"},
    ]

    first_bad: BadChar* = NULL
    first_bad_ptr: byte* = NULL
    for bad = &bad_chars[0]; bad < &bad_chars[sizeof(bad_chars)/sizeof(bad_chars[0])]; bad++:
        search_string = utf8_encode_char(bad.codepoint)
        bad_ptr = strstr(file_content, search_string)
        if bad_ptr != NULL and (first_bad_ptr == NULL or bad_ptr < first_bad_ptr):
            first_bad = bad
            first_bad_ptr = bad_ptr

    if first_bad != NULL and first_bad_ptr != NULL:
        lineno = 0
        for p = file_content; p < first_bad_ptr; p++:
            if *p == '\n':
                lineno++
        msg: byte[200]
        snprintf(msg, sizeof(msg), "source file contains Unicode whitespace character U+%08X %s", first_bad.codepoint, first_bad.name)
        fail(Location{path = path, lineno = lineno}, msg)


@public
class TokenizeResult:
    tokens: Token*

    # Content of the whole file as one big string.
    #
    # Cannot be freed before parsing, because the code included in `assert`
    # statements comes from here during parsing. It cannot be attached directly
    # to the tokens because we don't know where each assertion ends when
    # parsing. It is not necessarily a newline, because e.g. this is allowed:
    #
    #   for assert whatever; ...; ...:
    #       ...
    file_content: byte*

    def free(self) -> None:
        free_tokens(self.tokens)
        free(self.file_content)


@public
def tokenize(path: byte*, import_location: Location*) -> TokenizeResult:
    file_content = read_file_to_string(path, import_location)
    detect_bad_chars(path, file_content)
    raw_tokens = tokenize_without_indent_dedent_tokens(path, file_content)

    better_tokens = handle_indentations(raw_tokens)
    free(raw_tokens)
    return TokenizeResult{tokens = better_tokens, file_content = file_content}

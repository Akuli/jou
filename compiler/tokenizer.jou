import "stdlib/assert.jou"
import "stdlib/io.jou"
import "stdlib/list.jou"
import "stdlib/str.jou"
import "stdlib/mem.jou"
import "stdlib/errno.jou"
import "stdlib/ascii.jou"
import "./errors_and_warnings.jou"
import "./token.jou"

def is_identifier_or_number_byte(b: byte) -> bool:
    # Allows non-ASCII variable names (bytes 128-255)
    return b >= 128 or is_ascii_letter(b) or is_ascii_digit(b) or b == '_'

def is_operator_byte(c: byte) -> bool:
    return c != '\0' and strchr("=<>!.,()[]{};:+-*/&%|^~", c) != NULL

def is_keyword(word: byte*) -> bool:
    # See also: tests/should_succeed/keywords.jou
    match word with strcmp:
        case (
            "import" | "link"
            | "def" | "declare" | "class" | "union" | "enum" | "typedef" | "global" | "const"
            | "if" | "elif" | "else" | "while" | "for" | "pass" | "break" | "continue" | "match" | "with" | "case" | "return"
            | "True" | "False" | "None" | "NULL" | "self"
            | "and" | "or" | "not" | "as" | "sizeof" | "array_count" | "assert"
            | "bool" | "float" | "double" | "byte" | "int"
            | "int8" | "int16" | "int32" | "int64"| "uint8" | "uint16" | "uint32" | "uint64"
            | "void" | "noreturn" | "funcptr"
        ):
            return True
        case _:
            return False

def hexdigit_value(c: byte) -> int:
    if 'A' <= c and c <= 'F':
        return 10 + (c - 'A')
    if 'a' <= c and c <= 'f':
        return 10 + (c - 'a')
    if '0' <= c and c <= '9':
        return c - '0'
    return -1

def parse_integer(string: byte*, location: Location) -> uint64:
    if starts_with(string, "0b"):
        base = 2
        digits = &string[2]
        valid_digits = "01_"
    elif starts_with(string, "0o"):
        base = 8
        digits = &string[2]
        valid_digits = "01234567_"
    elif starts_with(string, "0x"):
        base = 16
        digits = &string[2]
        valid_digits = "0123456789ABCDEFabcdef_"
    else:
        # default decimal number
        base = 10
        digits = string
        valid_digits = "0123456789_"

    if (
        strlen(digits) == 0
        or digits[0] == '_'
        or digits[strlen(digits) - 1] == '_'
        or strspn(digits, valid_digits) != strlen(digits)
    ):
        message: byte*
        asprintf(&message, "invalid number or variable name \"%s\"", string)
        fail(location, message)

    if base == 10 and starts_with(string, "0") and strlen(string) >= 2:
        # wrong syntax like 0777
        fail(location, "unnecessary zero at start of number")

    # TODO: use strtoull() or similar? This isn't too bad written by hand though.
    result = 0 as uint64
    overflow = False

    for i = 0; i < strlen(digits); i++:
        if digits[i] == '_':
            continue
        digit_value = hexdigit_value(digits[i]) as uint64

        # Overflow isn't UB in Jou
        if (result * (base as uint64)) / (base as uint64) != result:
            overflow = True
            break
        result *= base as uint64

        if result + digit_value < result:
            overflow = True
            break
        result += digit_value

    if overflow:
        fail(location, "value does not fit into any supported integer type")

    return result


def is_valid_double(str: byte*) -> bool:
    # See doc/syntax-spec.md
    e = strstr(str, "e")
    if e == NULL:
        # One or more digits, then '.', then one or more digits
        n1 = strspn(str, "0123456789")
        if str[n1] != '.':
            return False
        n2 = strspn(&str[n1 + 1], "0123456789")
        return n1 > 0 and n2 > 0 and n1 + 1 + n2 == strlen(str)
    else:
        # First anything accepted above or just one or more digits
        n1 = strspn(str, "0123456789")
        if &str[n1] != e:
            if str[n1] != '.':
                return False
            n2 = strspn(&str[n1 + 1], "0123456789")
            if n1 == 0 or n2 == 0 or &str[n1 + 1 + n2] != e:
                return False
        # Then e, then possibly minus, then one or more digits
        p = e
        p++
        if *p == '-':
            p++
        return *p != '\0' and strspn(p, "0123456789") == strlen(p)

def is_valid_float(str: byte[100]) -> bool:
    n = strlen(str)
    if n == 0 or (str[n-1] != 'F' and str[n-1] != 'f'):
        return False
    str[n-1] = '\0'
    return is_valid_double(str) or strspn(str, "0123456789") == strlen(str)


def flip_paren(c: byte) -> byte:
    match c:
        case '(':
            return ')'
        case ')':
            return '('
        case '[':
            return ']'
        case ']':
            return '['
        case '{':
            return '}'
        case '}':
            return '{'
        case _:
            assert False


def detect_banned_whitespace_characters(last_3_bytes: byte[3]) -> byte*:
    last_2_bytes = [last_3_bytes[1], last_3_bytes[2]]

    # Ban all UTF-8 whitespace characters (longest one is 3 bytes)
    # Python code used to generate this:
    #
    #    import unicodedata
    #    for x in range(128, 0x110000):  # 128 to skip ascii, chr(0x110000) errors
    #        if chr(x).isspace():
    #            try:
    #                name = unicodedata.name(chr(x))
    #            except ValueError:
    #                name = "???"
    #            print(chr(x).encode('utf-8'), name)
    length2 = [
        ["\xc2\x85", "U+00000085 NEXT LINE (NEL)"],
        ["\xc2\xa0", "U+000000A0 NO-BREAK SPACE"],
    ]
    length3 = [
        ["\xe1\x9a\x80", "U+00001680 OGHAM SPACE MARK"],
        ["\xe2\x80\x80", "U+00002000 EN QUAD"],
        ["\xe2\x80\x81", "U+00002001 EM QUAD"],
        ["\xe2\x80\x82", "U+00002002 EN SPACE"],
        ["\xe2\x80\x83", "U+00002003 EM SPACE"],
        ["\xe2\x80\x84", "U+00002004 THREE-PER-EM SPACE"],
        ["\xe2\x80\x85", "U+00002005 FOUR-PER-EM SPACE"],
        ["\xe2\x80\x86", "U+00002006 SIX-PER-EM SPACE"],
        ["\xe2\x80\x87", "U+00002007 FIGURE SPACE"],
        ["\xe2\x80\x88", "U+00002008 PUNCTUATION SPACE"],
        ["\xe2\x80\x89", "U+00002009 THIN SPACE"],
        ["\xe2\x80\x8a", "U+0000200A HAIR SPACE"],
        ["\xe2\x80\xa8", "U+00002028 LINE SEPARATOR"],
        ["\xe2\x80\xa9", "U+00002029 PARAGRAPH SEPARATOR"],
        ["\xe2\x80\xaf", "U+0000202F NARROW NO-BREAK SPACE"],
        ["\xe2\x81\x9f", "U+0000205F MEDIUM MATHEMATICAL SPACE"],
        ["\xe3\x80\x80", "U+00003000 IDEOGRAPHIC SPACE"],
    ]

    for i = 0; i < sizeof(length2)/sizeof(length2[0]); i++:
        if memcmp(last_2_bytes, length2[i][0], 2) == 0:
            return length2[i][1]

    for i = 0; i < sizeof(length3)/sizeof(length3[0]); i++:
        if memcmp(last_3_bytes, length3[i][0], 3) == 0:
            return length3[i][1]

    return NULL


class Tokenizer:
    f: FILE*
    location: Location
    byte_offset: int
    pushback: List[byte]
    # Parens array isn't dynamic, so that you can't segfault
    # the compiler by feeding it lots of nested parentheses,
    # which would make it recurse too deep.
    open_parens: Token[50]
    open_parens_len: int
    last_3_bytes: byte[3]

    def read_byte(self) -> byte:
        c: byte
        if self.pushback.len > 0:
            c = self.pushback.pop()
            self.byte_offset++
        else:
            temp = fgetc(self.f)
            self.byte_offset++
            if temp == '\r':
                # On Windows, \r just before \n is ignored.
                temp = fgetc(self.f)
                self.byte_offset++
                if temp != EOF and temp != '\n':
                    # TODO: test this, if possible?
                    fail(self.location, "source file contains a CR byte ('\\r') that isn't a part of a CRLF line ending")

            if temp == EOF:
                if ferror(self.f) != 0:
                    # TODO: include errno in the error message
                    fail(self.location, "cannot read file")
                # Use the zero byte to denote end of file.
                c = '\0'
            elif temp == '\0':
                fail(self.location, "source file contains a zero byte")
            else:
                c = temp as byte
                self.last_3_bytes[0] = self.last_3_bytes[1]
                self.last_3_bytes[1] = self.last_3_bytes[2]
                self.last_3_bytes[2] = c
                bad = detect_banned_whitespace_characters(self.last_3_bytes)
                if bad != NULL:
                    msg: byte[500]
                    snprintf(msg, sizeof(msg), "source file contains Unicode whitespace character %s", bad)
                    fail(self.location, msg)

        if c == '\n':
            self.location.lineno++
        return c

    def unread_byte(self, b: byte) -> None:
        if b == '\0':
            return

        assert b != '\r'
        self.pushback.append(b)
        self.byte_offset--
        if b == '\n':
            self.location.lineno--

    def read_identifier_or_number(self, first_byte: byte) -> byte[100]:
        dest: byte[100]
        memset(&dest, 0, sizeof dest)
        destlen = 0

        assert is_identifier_or_number_byte(first_byte) or first_byte == '@'
        dest[destlen++] = first_byte
        is_number = '0' <= first_byte and first_byte <= '9'

        while True:
            b = self.read_byte()
            if (
                is_identifier_or_number_byte(b)
                or (is_number and (b == '.' or (b == '-' and dest[destlen-1] == 'e')))
            ):
                if destlen == sizeof dest - 1:
                    if is_number:
                        template = "number is too long: %.20s..."
                    else:
                        template = "name is too long: %.20s..."
                    message: byte[100]
                    sprintf(message, template, dest)
                    fail(self.location, message)
                dest[destlen++] = b
            else:
                self.unread_byte(b)
                return dest

    def consume_rest_of_line(self) -> None:
        while True:
            c = self.read_byte()
            if c == '\0' or c == '\n':
                self.unread_byte(c)
                break

    # Returns the indentation level for the next line
    def read_newline_token(self) -> int:
        level = 0
        while True:
            c = self.read_byte()
            match c:
                case '\0':
                    # End of file. Do not validate that indentation is a
                    # multiple of 4 spaces. Add a trailing newline implicitly
                    # if needed.
                    return 0
                case '\n':
                    level = 0
                case '#':
                    self.consume_rest_of_line()
                case ' ':
                    level++
                case _:
                    self.unread_byte(c)
                    return level

    def read_hex_escape_byte(self) -> byte:
        n1 = hexdigit_value(self.read_byte())
        n2 = hexdigit_value(self.read_byte())
        if n1 == -1 or n2 == -1:
            fail(self.location, "\\x must be followed by two hexadecimal digits (0-9, A-F) to specify a byte")
        return (n1*16 + n2) as byte

    # Assumes the initial ' has been read.
    def read_byte_literal(self) -> byte:
        c = self.read_byte()
        if c == '\'':
            fail(self.location, "a byte literal cannot be empty, maybe use double quotes to instead make a string?")
        if c == '\0' or c == '\n':
            if c == '\n':
                self.location.lineno--
            fail(self.location, "missing ' to end the byte literal")

        if c == '\\':
            after_backslash = self.read_byte()
            match after_backslash:
                case '\0':
                    fail(self.location, "missing ' to end the byte literal")
                case '\n':
                    self.location.lineno--
                    fail(self.location, "missing ' to end the byte literal")
                case 'n':
                    c = '\n'
                case 'r':
                    c = '\r'
                case 't':
                    c = '\t'
                case '\\':
                    c = '\\'
                case '\'':
                    c = '\''
                case '"':
                    fail(self.location, "double quotes shouldn't be escaped in byte literals")
                case '0':
                    c = '\0'
                case 'x':
                    c = self.read_hex_escape_byte()
                case _:
                    if is_ascii_printable(after_backslash):
                        message: byte* = malloc(100)
                        sprintf(message, "unknown escape: '\\%c'", after_backslash)
                        fail(self.location, message)
                    else:
                        fail(self.location, "unknown '\\' escape")

        end = self.read_byte()
        if end != '\'':
            # If there's another single quote later on the same line, suggest using double quotes.
            location = self.location
            while True:
                match self.read_byte():
                    case '\0' | '\n':
                        fail(location, "missing ' to end the byte literal")
                    case '\'':
                        fail(location, "single quotes are for specifying a byte, maybe use double quotes to instead make a string?")

        return c

    # Assumes the initial " has been read.
    def read_string(self) -> byte*:
        result = List[byte]{}
        msg: byte[100]

        while True:
            c = self.read_byte()
            match c:
                case '"':
                    break
                case '\n':
                    self.location.lineno--
                    fail(self.location, "missing \" to end the string")
                case '\0':
                    fail(self.location, "missing \" to end the string")
                case '\\':
                    # \n means newline, for example
                    after_backslash = self.read_byte()
                    match after_backslash:
                        case '\0':
                            fail(self.location, "missing \" to end the string")
                        case 'n':
                            result.append('\n')
                        case 'r':
                            result.append('\r')
                        case 't':
                            result.append('\t')
                        case '\\':
                            result.append('\\')
                        case '"':
                            result.append('"')
                        case '\'':
                            fail(self.location, "single quotes shouldn't be escaped in strings")
                        case '0':
                            fail(self.location, "strings cannot contain zero bytes (\\0), because that is the special end marker byte")
                        case 'x':
                            b = self.read_hex_escape_byte()
                            if b == '\0':
                                fail(self.location, "strings cannot contain zero bytes (\\x00), because that is the special end marker byte")
                            result.append(b)
                        case '\n':
                            # \ at end of line, string continues on next line
                            pass
                        case _:
                            if is_ascii_printable(after_backslash):
                                snprintf(msg, sizeof(msg), "unknown escape: '\\%c'", after_backslash)
                            else:
                                msg = "unknown '\\' escape"
                            fail(self.location, msg)
                case _:
                    result.append(c)

        result.append('\0')
        return result.ptr

    def read_operator(self) -> byte[100]:
        msg: byte[100]

        operators = [
            # Please update the syntax documentation if you add new operators.
            # Longer operators are first, so that '==' does not tokenize as '=' '='
            "...", "===", "!==", "<<=", ">>=",
            "==", "!=", "->", "<=", ">=", "++", "--", "+=", "-=", "*=", "/=", "%=", "&=", "|=", "^=",
            "&&", "||", "<<", ">>",
            ".", ",", ":", ";", "=", "(", ")", "{", "}", "[", "]", "&", "%", "*", "/", "+", "-", "^", "<", ">", "!", "|", "~",
        ]
        operator: byte[100] = ""

        # Read as many operator characters as we may need.
        while strlen(operator) < 3:
            c = self.read_byte()
            if not is_operator_byte(c):
                self.unread_byte(c)
                break
            operator[strlen(operator)] = c

        for i = 0; i < sizeof operators / sizeof operators[0]; i++:
            if starts_with(operator, operators[i]):
                # Unread the bytes we didn't use.
                while strlen(operator) > strlen(operators[i]):
                    last = &operator[strlen(operator) - 1]
                    self.unread_byte(*last)
                    *last = '\0'

                # These operators are here only to give a better error message when you
                # accidentally use syntax of another programming language.
                match operator with strcmp:
                    case "===":
                        fail(self.location, "use '==' instead of '==='")
                    case "!==":
                        fail(self.location, "use '!=' instead of '!=='")
                    case "&&":
                        fail(self.location, "use 'and' instead of '&&'")
                    case "||":
                        fail(self.location, "use 'or' instead of '||'")
                    case "!":
                        fail(self.location, "use 'not' instead of '!'")
                    case _:
                        return operator

        snprintf(msg, sizeof(msg), "there is no '%s' operator", operator)
        fail(self.location, msg)

    def handle_parentheses(self, token: Token*) -> None:
        msg: byte[100]

        if token.kind == TokenKind.EndOfFile and self.open_parens_len > 0:
            open_token = self.open_parens[0]
            actual_open = open_token.short_string[0]
            expected_close = flip_paren(actual_open)
            snprintf(msg, sizeof(msg), "'%c' without a matching '%c'", actual_open, expected_close)
            fail(open_token.location, msg)

        if token.is_open_paren():
            if self.open_parens_len == sizeof self.open_parens / sizeof self.open_parens[0]:
                fail(token.location, "too many nested parentheses")
            self.open_parens[self.open_parens_len++] = *token

        if token.is_close_paren():
            actual_close = token.short_string[0]
            expected_open = flip_paren(actual_close)
            if self.open_parens_len == 0 or self.open_parens[--self.open_parens_len].short_string[0] != expected_open:
                snprintf(msg, sizeof(msg), "'%c' without a matching '%c'", actual_close, expected_open)
                fail(token.location, msg)

    def read_token(self) -> Token:
        while True:
            token = Token{location = self.location, byte_offset = self.byte_offset}
            b = self.read_byte()
            match b:
                case ' ':
                    continue
                case '#':
                    self.consume_rest_of_line()
                    continue
                case '\n':
                    if self.open_parens_len > 0:
                        continue
                    token.kind = TokenKind.Newline
                    token.indentation_level = self.read_newline_token()
                case '"':
                    token.kind = TokenKind.String
                    token.long_string = self.read_string()
                case '\'':
                    token.kind = TokenKind.Byte
                    token.integer_value = self.read_byte_literal()
                case '@':
                    token.kind = TokenKind.Decorator
                    token.short_string = self.read_identifier_or_number(b)
                case '\t':
                    fail(self.location, "Jou files cannot contain tab characters (use 4 spaces for indentation)")
                case '\0':
                    token.kind = TokenKind.EndOfFile
                case _:
                    if is_identifier_or_number_byte(b):
                        token.short_string = self.read_identifier_or_number(b)
                        if is_keyword(token.short_string):
                            token.kind = TokenKind.Keyword
                        elif '0' <= token.short_string[0] and token.short_string[0] <= '9':
                            if is_valid_double(token.short_string):
                                token.kind = TokenKind.Double
                            elif is_valid_float(token.short_string):
                                token.short_string[strlen(token.short_string) - 1] = '\0'  # delete trailing 'f' or 'F'
                                token.kind = TokenKind.Float
                            else:
                                token.kind = TokenKind.Integer
                                token.integer_value = parse_integer(token.short_string, token.location)
                        else:
                            token.kind = TokenKind.Name
                    elif is_operator_byte(b):
                        self.unread_byte(b)
                        token.kind = TokenKind.Operator
                        token.short_string = self.read_operator()
                    else:
                        message: byte[100]
                        if is_ascii_printable(b):
                            sprintf(message, "unexpected byte '%c' (%#02x)", b, b)
                        else:
                            sprintf(message, "unexpected byte %#02x", b)
                        fail(self.location, message)

            self.handle_parentheses(&token)
            return token


def tokenize_without_indent_dedent_tokens(file: FILE*, path: byte*) -> Token*:
    tokenizer = Tokenizer{
        location = Location{path = path},
        byte_offset = -1,  # Compensate for fake newline added below
        f = file,
    }

    # Add a fake newline to the beginning. It does a few things:
    #  * Less special-casing: blank lines in the beginning of the file can
    #    cause there to be a newline token anyway.
    #  * It is easier to detect an unexpected indentation in the beginning
    #    of the file, as it becomes just like any other indentation.
    #  * Line numbers start at 1.
    tokenizer.pushback.append('\n')

    tokens = List[Token]{}
    while tokens.len == 0 or tokens.end()[-1].kind != TokenKind.EndOfFile:
        tokens.append(tokenizer.read_token())

    free(tokenizer.pushback.ptr)
    return tokens.ptr


# Creates a new array of tokens with indent/dedent tokens added after
# newline tokens that change the indentation level.
def handle_indentations(raw_tokens: Token*) -> Token*:
    tokens = List[Token]{}
    level = 0

    for t = raw_tokens; True; t++:
        if t.kind == TokenKind.EndOfFile:
            # Add an extra newline token at end of file and the dedents after it.
            # This makes it similar to how other newline and dedent tokens work:
            # the dedents always come after a newline token.
            if t > raw_tokens and t[-1].kind != TokenKind.Newline:
                tokens.append(Token{location = t.location, byte_offset = t.byte_offset, kind = TokenKind.Newline})
            while level != 0:
                tokens.append(Token{location = t.location, byte_offset = t.byte_offset, kind = TokenKind.Dedent})
                level -= 4
            tokens.append(*t)
            break

        tokens.append(*t)

        if t.kind == TokenKind.Newline:
            after_newline = t.location
            after_newline.lineno++
            byte_offset = t.byte_offset + 1

            if t.indentation_level % 4 != 0:
                fail(after_newline, "indentation must be a multiple of 4 spaces")

            while level < t.indentation_level:
                tokens.append(Token{location = after_newline, byte_offset = byte_offset, kind = TokenKind.Indent})
                level += 4

            while level > t.indentation_level:
                tokens.append(Token{location = after_newline, byte_offset = byte_offset, kind = TokenKind.Dedent})
                level -= 4

    # Delete the newline token in the beginning.
    #
    # If the file has indentations after it, they are now represented by separate
    # indent tokens and parsing will fail. If the file doesn't have any blank/comment
    # lines in the beginning, it has a newline token anyway to avoid special casing.
    assert tokens.ptr[0].kind == TokenKind.Newline
    memmove(&tokens.ptr[0], &tokens.ptr[1], sizeof tokens.ptr[0] * --tokens.len)

    return tokens.ptr


@public
def tokenize(path: byte*, import_location: Location*) -> Token*:
    file = fopen(path, "rb")
    if file == NULL:
        message: byte[200]
        if import_location == NULL:
            # File is not imported
            snprintf(message, sizeof message, "cannot open file: %s", strerror(get_errno()))
            fail(Location{path=path}, message)
        else:
            snprintf(message, sizeof message, "cannot import from \"%s\": %s", path, strerror(get_errno()))
            fail(*import_location, message)

    raw_tokens = tokenize_without_indent_dedent_tokens(file, path)
    fclose(file)

    better_tokens = handle_indentations(raw_tokens)
    free(raw_tokens)
    return better_tokens

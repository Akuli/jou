import "stdlib/io.jou"

def main() -> int:
    # Output: Hello
    putchar(72 as byte)
    putchar(357 as byte)
    putchar(25600108 as byte)
    putchar(-148 as byte)
    putchar(-2449 as byte)
    putchar('\n')

    printf("%#x\n", *("aaaa" as int*))  # Output: 0x61616161

    printf("%.2f\n", 12.34f as double)  # Output: 12.34
    printf("%.2f\n", 12.34 as float)  # Output: 12.34

    printf("%.2f\n", 123 as float)  # Output: 123.00

    printf("%d\n", 12.34 as int)  # Output: 12
    printf("%d\n", 12.99 as int)  # Output: 12
    printf("%d\n", -12.34 as int)  # Output: -12
    printf("%d\n", -12.99 as int)  # Output: -12

    printf("%d\n", 12.34 as byte)  # Output: 12
    printf("%d\n", 12.99 as int)  # Output: 12

    printf("%d\n", True as int)  # Output: 1
    printf("%d\n", False as int)  # Output: 0

    # This too is usually done with type inference. Value is too large for int, inferred as int64
    n = 123123123123123 as int64
    printf("%lld\n", n)  # Output: 123123123123123
    printf("%d bytes (%d bits)\n", sizeof(n), sizeof(n)*8)  # Output: 8 bytes (64 bits)

    # If it's already the right type, the cast does nothing.
    #
    # We don't show a warning for these casts, because these are useful when
    # the type is somehow platform-dependent. For example, if the type of `x`
    # is `intnative`, then `x as int` and `x as int64` should succeed without
    # warnings on 32-bit and 64-bit systems.
    printf("%d\n", 1 as int)  # Output: 1

    return 0

import "stdlib/io.jou"

def main() -> int:
    # Output: Hello
    putchar(72 as byte)
    putchar(357 as byte)
    putchar(25600108 as byte)
    putchar(-148 as byte)
    putchar(-2449 as byte)
    putchar('\n')

    printf("%#x\n", *("aaaa" as int*))  # Output: 0x61616161

    printf("%.2f\n", 12.34f as double)  # Output: 12.34
    printf("%.2f\n", 12.34 as float)  # Output: 12.34

    printf("%.2f\n", 123 as float)  # Output: 123.00

    printf("%d\n", 12.34 as int)  # Output: 12
    printf("%d\n", 12.99 as int)  # Output: 12
    printf("%d\n", -12.34 as int)  # Output: -12
    printf("%d\n", -12.99 as int)  # Output: -12

    printf("%d\n", 12.34 as byte)  # Output: 12
    printf("%d\n", 12.99 as int)  # Output: 12

    printf("%d\n", True as int)  # Output: 1
    printf("%d\n", False as int)  # Output: 0

    # This too is usually done with type inference. Value is too large for int, inferred as int64
    n = 123123123123123 as int64
    printf("%lld\n", n)  # Output: 123123123123123
    printf("%d bytes (%d bits)\n", sizeof(n), sizeof(n)*8)  # Output: 8 bytes (64 bits)

    one = 1 as int  # Warning: unnecessary cast from int to int
    printf("%d\n", one)  # Output: 1

    # Not unnecessary cast, otherwise y would become int
    y = 123 as int64
    printf("%lld\n", y)  # Output: 123

    return 0
